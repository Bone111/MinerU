# MinerU 2.5.3 V100 vLLM æ”¯æŒæŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜äº†Tesla V100 GPUå¯¹MinerU 2.5.3ç‰ˆæœ¬ä¸­vLLMæ”¯æŒçš„æƒ…å†µï¼ŒåŒ…æ‹¬å…¼å®¹æ€§è¦æ±‚ã€éƒ¨ç½²æ–¹æ¡ˆã€ä¼˜åŒ–é…ç½®å’Œæœ€ä½³å®è·µã€‚

### ğŸ¯ å…³é”®ä¿¡æ¯

- **GPUå‹å·**: Tesla V100-SXM2-32GB
- **æ¶æ„**: Volta (Compute Capability 7.0)
- **vLLMæ”¯æŒ**: âœ… å®Œå…¨æ”¯æŒï¼Œéœ€è¦ç‰¹å®šç‰ˆæœ¬
- **æ¨èé•œåƒ**: `docker.m.daocloud.io/vllm/vllm-openai:v0.10.2`

## âœ… **V100 vLLMæ”¯æŒçŠ¶æ€**

### **å…¼å®¹æ€§çŸ©é˜µ**

| é•œåƒç‰ˆæœ¬ | V100æ”¯æŒ | æ¶æ„è¦æ±‚ | è¯´æ˜ |
|----------|----------|----------|------|
| `vllm/vllm-openai:v0.10.1.1` | âŒ **ä¸æ”¯æŒ** | CCâ‰¥8.0 (Ampere+) | ä»…æ”¯æŒRTX 30/40ç³»åˆ—ã€A100ç­‰æ–°æ¶æ„ |
| `vllm/vllm-openai:v0.10.2` | âœ… **æ”¯æŒ** | CCâ‰¥7.0 (Volta+) | **V100æ¨èç‰ˆæœ¬** |
| `docker.m.daocloud.io/vllm/vllm-openai:v0.10.2` | âœ… **æ”¯æŒ** | CCâ‰¥7.0 (Volta+) | **å›½å†…é•œåƒï¼Œæ¨è** |

### **æ¶æ„åˆ†çº§è¯´æ˜**

| GPUæ¶æ„ | Compute Capability | ä»£è¡¨GPU | vLLMç‰ˆæœ¬è¦æ±‚ | çŠ¶æ€ |
|---------|-------------------|---------|-------------|------|
| **Ampere** | 8.0, 8.6, 8.7 | RTX 30/40ç³»åˆ—, A100, H100 | v0.10.1.1+ | âœ… å®Œå…¨æ”¯æŒ |
| **Turing** | 7.5 | RTX 20ç³»åˆ—, GTX 16ç³»åˆ—, T4 | v0.10.2 | âœ… éœ€ä¿®æ”¹é…ç½® |
| **Volta** | 7.0 | **V100**, Titan V | v0.10.2 | âœ… **éœ€ä¿®æ”¹é…ç½®** |
| **PascalåŠä»¥ä¸‹** | <7.0 | P100, GTX 10ç³»åˆ— | ä¸æ”¯æŒ | âŒ å»ºè®®pipelineåç«¯ |

## ğŸš€ éƒ¨ç½²æ–¹æ¡ˆ

### **æ–¹æ¡ˆä¸€ï¼šæ„å»ºV100ä¸“ç”¨é•œåƒï¼ˆæ¨èï¼‰**

#### 1. å‡†å¤‡Dockerfile

```dockerfile
# V100ä¸“ç”¨é…ç½® - ä½¿ç”¨vLLM v0.10.2ç‰ˆæœ¬
# é€‚ç”¨äºVoltaæ¶æ„GPU (Compute Capability 7.0)
FROM docker.m.daocloud.io/vllm/vllm-openai:v0.10.2

# æ³¨é‡Šæ‰ä¸å…¼å®¹çš„é•œåƒç‰ˆæœ¬ï¼ˆä»…æ”¯æŒAmpereæ¶æ„ï¼‰
# FROM docker.m.daocloud.io/vllm/vllm-openai:v0.10.1.1

# å®‰è£…ç³»ç»Ÿä¾èµ–å’Œä¸­æ–‡å­—ä½“æ”¯æŒ
RUN apt-get update && \
    apt-get install -y \
        fonts-noto-core \
        fonts-noto-cjk \
        fontconfig \
        libgl1 \
        curl \
        htop \
        nvtop \
        vim && \
    fc-cache -fv && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# å®‰è£…MinerUï¼ˆä½¿ç”¨å›½å†…é•œåƒæºåŠ é€Ÿï¼‰
RUN python3 -m pip install -U 'mineru[core]' \
    -i https://mirrors.aliyun.com/pypi/simple \
    --break-system-packages && \
    python3 -m pip cache purge

# ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆä½¿ç”¨ModelScopeå›½å†…é•œåƒï¼‰
RUN /bin/bash -c "mineru-models-download -s modelscope -m all"

# è®¾ç½®V100ä¼˜åŒ–çš„ç¯å¢ƒå˜é‡
ENV CUDA_VISIBLE_DEVICES=0,1
ENV OMP_NUM_THREADS=16  
ENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,expandable_segments:True
ENV VLLM_WORKER_MULTIPROC_METHOD=spawn

# åˆ›å»ºå·¥ä½œç›®å½•
WORKDIR /app
RUN mkdir -p /app/output /app/logs /app/models /app/input

# è®¾ç½®å…¥å£ç‚¹
ENTRYPOINT ["/bin/bash", "-c", "export MINERU_MODEL_SOURCE=local && exec \"$@\"", "--"]
```

#### 2. æ„å»ºé•œåƒ

```bash
# åœ¨MinerUé¡¹ç›®ç›®å½•ä¸‹
cd /path/to/MinerU/docker/china

# æ›¿æ¢Dockerfileå†…å®¹
cp Dockerfile Dockerfile.backup
# å°†ä¸Šè¿°Dockerfileå†…å®¹å†™å…¥Dockerfileæ–‡ä»¶

# æ„å»ºV100ä¸“ç”¨é•œåƒ
docker build -t mineru-v100:2.5.3 .

# éªŒè¯é•œåƒæ„å»ºæˆåŠŸ
docker images | grep mineru-v100:2.5.3
```

### **æ–¹æ¡ˆäºŒï¼šç›´æ¥ä½¿ç”¨é¢„æ„å»ºé•œåƒ**

å¦‚æœå·²æœ‰V100å…¼å®¹çš„é¢„æ„å»ºé•œåƒï¼š

```bash
# æ‹‰å–V100å…¼å®¹é•œåƒ
docker pull docker.m.daocloud.io/vllm/vllm-openai:v0.10.2

# æˆ–ä½¿ç”¨å®˜æ–¹é•œåƒ
docker pull vllm/vllm-openai:v0.10.2
```

## âš¡ V100 vLLMæœåŠ¡å¯åŠ¨

### **å•GPUé…ç½®ï¼ˆæ¨èï¼‰**

```bash
# åˆ›å»ºå·¥ä½œç›®å½•
mkdir -p ~/mineru-v100-workspace/{output,logs,models}
cd ~/mineru-v100-workspace

# å¯åŠ¨V100 vLLMæœåŠ¡å™¨
docker run -d --restart=unless-stopped \
  --name mineru-vllm-v100 \
  --gpus '"device=1"' \
  -p 30000:30000 \
  -v $(pwd)/output:/app/output \
  -v $(pwd)/logs:/app/logs \
  -v $(pwd)/models:/app/models \
  --memory=24g \
  --cpus=8 \
  --shm-size=8g \
  -e CUDA_VISIBLE_DEVICES=1 \
  -e VLLM_WORKER_MULTIPROC_METHOD=spawn \
  -e PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,expandable_segments:True \
  -e LOG_LEVEL=info \
  mineru-v100:2.5.3 \
  mineru-vllm-server \
    --host 0.0.0.0 \
    --port 30000 \
    --gpu-memory-utilization 0.7 \
    --max-model-len 8192 \
    --trust-remote-code
```

### **åŒGPUé…ç½®ï¼ˆé«˜æ€§èƒ½ï¼‰**

```bash
# Tesla V100åŒGPUé…ç½®
docker run -d --restart=unless-stopped \
  --name mineru-vllm-v100-dual \
  --gpus all \
  -p 30000:30000 \
  -v $(pwd)/output:/app/output \
  -v $(pwd)/logs:/app/logs \
  -v $(pwd)/models:/app/models \
  --memory=48g \
  --cpus=16 \
  --shm-size=16g \
  -e CUDA_VISIBLE_DEVICES=0,1 \
  -e VLLM_WORKER_MULTIPROC_METHOD=spawn \
  -e PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024,expandable_segments:True \
  mineru-v100:2.5.3 \
  mineru-vllm-server \
    --host 0.0.0.0 \
    --port 30000 \
    --gpu-memory-utilization 0.8 \
    --tensor-parallel-size 2 \
    --max-model-len 16384 \
    --trust-remote-code
```

### **APIæœåŠ¡é…ç½®ï¼ˆè¿æ¥vLLMï¼‰**

```bash
# å¯åŠ¨APIæœåŠ¡ï¼ˆä½¿ç”¨vLLMåç«¯ï¼‰
docker run -d --restart=unless-stopped \
  --name mineru-api-v100 \
  -p 8000:8000 \
  -v $(pwd)/output:/app/output \
  -v $(pwd)/logs:/app/logs \
  --memory=8g \
  --cpus=4 \
  -e WORKERS=4 \
  -e LOG_LEVEL=info \
  mineru-v100:2.5.3 \
  mineru-api --host 0.0.0.0 --port 8000 --workers 4
```

## âš™ï¸ V100ä¼˜åŒ–é…ç½®

### **å…³é”®å‚æ•°è¯´æ˜**

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `--gpu-memory-utilization` | 0.6-0.8 | GPUæ˜¾å­˜ä½¿ç”¨ç‡ï¼ŒV100å»ºè®®0.7 |
| `--tensor-parallel-size` | 1æˆ–2 | åŒV100æ—¶å¯è®¾ä¸º2 |
| `--max-model-len` | 8192-16384 | æœ€å¤§åºåˆ—é•¿åº¦ |
| `--trust-remote-code` | å¿…éœ€ | ä¿¡ä»»è¿œç¨‹ä»£ç æ‰§è¡Œ |
| `VLLM_WORKER_MULTIPROC_METHOD` | spawn | V100å¤šè¿›ç¨‹æ–¹æ³• |
| `PYTORCH_CUDA_ALLOC_CONF` | max_split_size_mb:512 | CUDAå†…å­˜åˆ†é…ä¼˜åŒ– |

### **æ€§èƒ½è°ƒä¼˜å»ºè®®**

```bash
# å•V100ä¼˜åŒ–é…ç½®
--gpu-memory-utilization 0.7 \
--max-model-len 8192 \
--swap-space 4 \
--disable-log-requests \
--trust-remote-code

# åŒV100ä¼˜åŒ–é…ç½®  
--gpu-memory-utilization 0.8 \
--tensor-parallel-size 2 \
--max-model-len 16384 \
--pipeline-parallel-size 1 \
--trust-remote-code
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### **ä¸åŒåç«¯æ€§èƒ½è¡¨ç°**

| åç«¯ç±»å‹ | V100å•å¡æ€§èƒ½ | V100åŒå¡æ€§èƒ½ | ç‰¹ç‚¹ |
|----------|--------------|--------------|------|
| **Pipeline** | åŸºå‡†æ€§èƒ½ | - | ç¨³å®šå…¼å®¹ï¼ŒCPU+GPUæ··åˆ |
| **VLM-transformers** | 1.5-2xåŸºå‡† | - | çº¯GPUæ¨ç†ï¼Œå†…å­˜å ç”¨å¤§ |
| **VLM-vLLM** | **3-5xåŸºå‡†** | **5-8xåŸºå‡†** | **é«˜æ€§èƒ½æ¨ç†åŠ é€Ÿ** âœ¨ |

### **é¢„æœŸå¤„ç†æ—¶é—´ï¼ˆ33é¡µæ–‡æ¡£ï¼‰**

| æ¨¡å¼ | å•V100 | åŒV100 | CPUæ¨¡å¼å¯¹æ¯” |
|------|--------|--------|-------------|
| **å¸ƒå±€åˆ†æ** | ~30ç§’ | ~20ç§’ | ~180ç§’ |
| **OCRè¯†åˆ«** | ~60ç§’ | ~40ç§’ | ~300ç§’ |
| **æ€»å¤„ç†æ—¶é—´** | **~2-3åˆ†é’Ÿ** | **~1.5-2åˆ†é’Ÿ** | ~10-15åˆ†é’Ÿ |

## ğŸ”§ æœåŠ¡ç®¡ç†

### **å¥åº·æ£€æŸ¥**

```bash
# æ£€æŸ¥vLLMæœåŠ¡çŠ¶æ€
curl http://localhost:30000/health

# æ£€æŸ¥å¯ç”¨æ¨¡å‹
curl http://localhost:30000/v1/models

# æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ
nvidia-smi

# æŸ¥çœ‹å®¹å™¨æ—¥å¿—
docker logs -f mineru-vllm-v100
```

### **æœåŠ¡æ§åˆ¶**

```bash
# å¯åŠ¨æœåŠ¡
docker start mineru-vllm-v100

# åœæ­¢æœåŠ¡
docker stop mineru-vllm-v100

# é‡å¯æœåŠ¡
docker restart mineru-vllm-v100

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker ps | grep mineru-vllm-v100
```

### **èµ„æºç›‘æ§**

```bash
# å®æ—¶GPUç›‘æ§
watch -n 1 nvidia-smi

# å®¹å™¨èµ„æºä½¿ç”¨
docker stats mineru-vllm-v100

# è¯¦ç»†GPUä¿¡æ¯
nvidia-smi --query-gpu=name,compute_cap,memory.total,memory.used,utilization.gpu,temperature.gpu --format=csv
```

## ğŸ§ª ä½¿ç”¨ç¤ºä¾‹

### **åŸºç¡€APIè°ƒç”¨**

```bash
# ä½¿ç”¨vLLMåç«¯è§£ææ–‡æ¡£
curl -X POST "http://localhost:8000/file_parse" \
  -H "Content-Type: multipart/form-data" \
  -F "files=@document.pdf" \
  -F "backend=vlm" \
  -F "server_url=http://localhost:30000/v1" \
  -F "lang_list=ch" \
  -F "return_md=true" \
  --max-time 300
```

### **Pythonå®¢æˆ·ç«¯**

```python
import requests

def parse_with_vllm(pdf_path, server_url="http://localhost:8000"):
    """ä½¿ç”¨V100 vLLMåç«¯è§£æPDF"""
    
    with open(pdf_path, 'rb') as f:
        files = {'files': f}
        data = {
            'backend': 'vlm',
            'server_url': 'http://localhost:30000/v1',
            'lang_list': 'ch',
            'return_md': 'true',
            'formula_enable': 'true',
            'table_enable': 'true'
        }
        
        response = requests.post(
            f"{server_url}/file_parse", 
            files=files, 
            data=data,
            timeout=300
        )
    
    if response.status_code == 200:
        result = response.json()
        print("âœ… è§£ææˆåŠŸ")
        return result
    else:
        print(f"âŒ è§£æå¤±è´¥: {response.status_code}")
        print(response.text)
        return None

# ä½¿ç”¨ç¤ºä¾‹
result = parse_with_vllm("test.pdf")
```

## ğŸš¨ æ•…éšœæ’é™¤

### **å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ**

#### 1. **CUDAæ¶æ„ä¸å…¼å®¹é”™è¯¯**

```
error: no kernel image is available for execution on the device
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# ç¡®è®¤ä½¿ç”¨v0.10.2é•œåƒ
docker inspect mineru-vllm-v100 | grep Image

# é‡å»ºå®¹å™¨ä½¿ç”¨æ­£ç¡®é•œåƒ
docker stop mineru-vllm-v100
docker rm mineru-vllm-v100
# ä½¿ç”¨v0.10.2é•œåƒé‡æ–°å¯åŠ¨
```

#### 2. **vLLMæœåŠ¡å¯åŠ¨å¤±è´¥**

```bash
# æ£€æŸ¥æ—¥å¿—
docker logs mineru-vllm-v100

# å¸¸è§åŸå› åŠè§£å†³æ–¹æ¡ˆ
# - GPUå†…å­˜ä¸è¶³ï¼šé™ä½gpu-memory-utilization
# - æ¨¡å‹åŠ è½½å¤±è´¥ï¼šæ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
# - ç«¯å£å†²çªï¼šæ›´æ¢ç«¯å£
```

#### 3. **æ€§èƒ½ä¸å¦‚é¢„æœŸ**

```bash
# æ£€æŸ¥GPUä½¿ç”¨ç‡
nvidia-smi

# ä¼˜åŒ–å»ºè®®
# 1. å¢åŠ gpu-memory-utilizationåˆ°0.8
# 2. ä½¿ç”¨åŒGPU tensor-parallel-size=2
# 3. è°ƒæ•´max-model-lené€‚åº”æ–‡æ¡£é•¿åº¦
```

#### 4. **å†…å­˜æº¢å‡ºï¼ˆOOMï¼‰**

```bash
# è§£å†³æ–¹æ¡ˆ
# 1. é™ä½gpu-memory-utilization
# 2. å‡å°‘max-model-len
# 3. å¯ç”¨swap-space
# 4. é‡å¯å®¹å™¨æ¸…ç†æ˜¾å­˜

docker restart mineru-vllm-v100
```

### **æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•**

- [ ] ç¡®è®¤ä½¿ç”¨v0.10.2é•œåƒç‰ˆæœ¬
- [ ] GPUå†…å­˜åˆ©ç”¨ç‡è®¾ç½®åœ¨0.6-0.8ä¹‹é—´
- [ ] åŒGPUæ—¶å¯ç”¨tensor-parallel-size=2
- [ ] è®¾ç½®åˆé€‚çš„max-model-len
- [ ] å¯ç”¨CUDAå†…å­˜ä¼˜åŒ–ç¯å¢ƒå˜é‡
- [ ] ç›‘æ§GPUæ¸©åº¦é¿å…è¿‡çƒ­é™é¢‘

## ğŸ“‹ æœ€ä½³å®è·µ

### **ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å»ºè®®**

1. **èµ„æºé…ç½®**
   - å•V100ï¼š24GBå†…å­˜ï¼Œ8CPUæ ¸å¿ƒ
   - åŒV100ï¼š48GBå†…å­˜ï¼Œ16CPUæ ¸å¿ƒ
   - SSDå­˜å‚¨ç”¨äºæ¨¡å‹å’Œè¾“å‡ºæ–‡ä»¶

2. **ç›‘æ§å‘Šè­¦**
   ```bash
   # è®¾ç½®GPUæ¸©åº¦ç›‘æ§
   watch -n 30 "nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader,nounits"
   
   # è®¾ç½®å†…å­˜ä½¿ç”¨ç›‘æ§
   docker stats --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}"
   ```

3. **å®šæœŸç»´æŠ¤**
   ```bash
   # æ¯æ—¥é‡å¯æœåŠ¡ï¼ˆå¯é€‰ï¼‰
   docker restart mineru-vllm-v100
   
   # æ¸…ç†è¾“å‡ºæ–‡ä»¶
   find ~/mineru-v100-workspace/output -mtime +7 -delete
   
   # æ›´æ–°é•œåƒ
   docker pull docker.m.daocloud.io/vllm/vllm-openai:v0.10.2
   ```

## ğŸ¯ ç»“è®º

Tesla V100å®Œå…¨æ”¯æŒMinerU 2.5.3çš„vLLMåŠ é€ŸåŠŸèƒ½ï¼Œå…³é”®åœ¨äºï¼š

1. âœ… **ä½¿ç”¨æ­£ç¡®çš„é•œåƒç‰ˆæœ¬**: `v0.10.2`è€Œé`v0.10.1.1`
2. âœ… **ä¼˜åŒ–é…ç½®å‚æ•°**: ç‰¹åˆ«æ˜¯GPUå†…å­˜åˆ©ç”¨ç‡å’Œå¹¶è¡Œè®¾ç½®
3. âœ… **åŒGPUé…ç½®**: å¯è·å¾—5-8å€æ€§èƒ½æå‡
4. âœ… **æŒç»­ç›‘æ§**: ç¡®ä¿æœåŠ¡ç¨³å®šé«˜æ•ˆè¿è¡Œ

ç›¸æ¯”CPUæ¨¡å¼ï¼ŒV100 vLLMå¯ä»¥æä¾›ï¼š
- **5-10å€å¤„ç†é€Ÿåº¦æå‡**
- **æ›´å¥½çš„èµ„æºåˆ©ç”¨ç‡**
- **æ”¯æŒæ›´å¤§æ–‡æ¡£å’Œæ‰¹é‡å¤„ç†**

**å¼ºçƒˆå»ºè®®ä»CPUæ¨¡å¼åˆ‡æ¢åˆ°V100 vLLMéƒ¨ç½²ï¼**

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æ›´æ–°æ—¥æœŸ**: 2024å¹´9æœˆ24æ—¥  
**é€‚ç”¨ç‰ˆæœ¬**: MinerU 2.5.3  
**ç¡¬ä»¶è¦æ±‚**: Tesla V100 (Voltaæ¶æ„, CC 7.0)

